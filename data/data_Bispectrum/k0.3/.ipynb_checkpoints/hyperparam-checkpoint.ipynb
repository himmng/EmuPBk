{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras as ks\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.constraints import maxnorm\n",
    "from keras.layers import Dropout\n",
    "from keras.optimizers import Adamax,Adam\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nod = 1058          #===========no. of data points============#\n",
    "k1 = np.array([0.1903934, 0.3220935, 1.559453 ])\n",
    "k2byk1 = np.arange(0.50,1.05,0.05) \t\t#======Ratio k2byk1========#\n",
    "\n",
    "cosalpha = np.arange(0.50,1.00,0.01)\t#======cosine of the angle between the k2 and k1 arms =======#\n",
    "\n",
    "k2byk1 = k2byk1.reshape(11,1)\n",
    "\n",
    "costheta = -cosalpha\n",
    "\n",
    "B_02 = np.loadtxt('bk_norm03')\n",
    "params_02 = np.loadtxt('params03')\n",
    "\n",
    "\n",
    "B_02 = B_02/1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = np.loadtxt('index')\n",
    "index = np.zeros(len(ind),dtype = int)\n",
    "for i in range(len(ind)):\n",
    "    index[i] = ind[i]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_test = B_02[index]\n",
    "B_train = np.delete(B_02,index,axis=0)\n",
    "\n",
    "p_test = params_02[index]\n",
    "p_train = np.delete(params_02,index,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1040, 550)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Could not interpret optimizer identifier: <class 'keras.optimizers.Adam'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-87ee70b7c5f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# Compile model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdamax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta_1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta_2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.088\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mse'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36msymbolic_fn_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_SYMBOLIC_SCOPE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mget_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0;31m`\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mor\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0msample_weight_mode\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \"\"\"\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compile_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/optimizers.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(identifier)\u001b[0m\n\u001b[1;32m    871\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m         raise ValueError('Could not interpret optimizer identifier: ' +\n\u001b[0;32m--> 873\u001b[0;31m                          str(identifier))\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: Could not interpret optimizer identifier: <class 'keras.optimizers.Adam'>"
     ]
    }
   ],
   "source": [
    "init_mode='uniform'\n",
    "class myCallback(ks.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if(logs.get('accuracy')>=0.98):\n",
    "            print(\"\\nReached 80% accuracy so cancelling training!\")\n",
    "            self.model.stop_training = True\n",
    "callbacks = myCallback()\n",
    "model = Sequential()\n",
    "model.add(Dense(80, input_shape=[3,], activation='elu',kernel_initializer=init_mode,))\n",
    "model.add(Dense(320,activation='elu',kernel_initializer=init_mode))\n",
    "model.add(Dense(460,activation='elu',kernel_initializer=init_mode))\n",
    "model.add(Dense(560,activation='elu',kernel_initializer=init_mode))\n",
    "model.add(Dense(260,activation='elu',kernel_initializer=init_mode))\n",
    "model.add(Dense(100,activation='elu',kernel_initializer=init_mode))\n",
    "model.add(Dense(320,activation='elu',kernel_initializer=init_mode))\n",
    "model.add(Dense(400,activation='elu',kernel_initializer=init_mode))\n",
    "\n",
    "    #model.add(Dropout(dropout_rate))\n",
    "model.add(Dense(550, activation='linear'))\n",
    "    # Compile model\n",
    "optimizer = Adamax(lr=0.0001, beta_1=0.2, beta_2=0.088)\n",
    "model.compile(loss='mse', optimizer=Adam, metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(p_train,B_train,validation_split=0.038,batch_size=20,epochs=2000,callbacks=[callbacks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCallback(ks.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if(logs.get('accuracy')>=0.98):\n",
    "            print(\"\\nReached 95% accuracy so cancelling training!\")\n",
    "            self.model.stop_training = True\n",
    "callbacks = myCallback()\n",
    "model = Sequential()\n",
    "model.add(Dense(80,input_shape=[3,],activation='elu'))\n",
    "model.add(Dense(320,activation='elu'))\n",
    "model.add(Dense(460,activation='relu'))\n",
    "model.add(Dense(560,activation='elu'))\n",
    "model.add(Dense(260,activation='elu'))\n",
    "model.add(Dense(100,activation='elu'))\n",
    "model.add(Dense(11*50))\n",
    "optimizer = Adam(lr = 0.0001,)   \n",
    "model.compile(loss='mse', optimizer= optimizer, metrics=['accuracy'],)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 40 samples\n",
      "Epoch 1/5000\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 13.4787 - accuracy: 0.2070 - val_loss: 10.1632 - val_accuracy: 0.4250\n",
      "Epoch 2/5000\n",
      "1000/1000 [==============================] - 1s 812us/step - loss: 8.8410 - accuracy: 0.6480 - val_loss: 6.6945 - val_accuracy: 0.8500\n",
      "Epoch 3/5000\n",
      "1000/1000 [==============================] - 1s 959us/step - loss: 5.2053 - accuracy: 0.7740 - val_loss: 5.5265 - val_accuracy: 0.8500\n",
      "Epoch 4/5000\n",
      "1000/1000 [==============================] - 1s 948us/step - loss: 4.1348 - accuracy: 0.7580 - val_loss: 5.2927 - val_accuracy: 0.8250\n",
      "Epoch 5/5000\n",
      "1000/1000 [==============================] - 1s 964us/step - loss: 3.3727 - accuracy: 0.7360 - val_loss: 4.9572 - val_accuracy: 0.8250\n",
      "Epoch 6/5000\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 3.0930 - accuracy: 0.7490 - val_loss: 4.7866 - val_accuracy: 0.8000\n",
      "Epoch 7/5000\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 2.7222 - accuracy: 0.7410 - val_loss: 4.7590 - val_accuracy: 0.8000\n",
      "Epoch 8/5000\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 2.6865 - accuracy: 0.7280 - val_loss: 4.5676 - val_accuracy: 0.8000\n",
      "Epoch 9/5000\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 2.4472 - accuracy: 0.7460 - val_loss: 4.5871 - val_accuracy: 0.8250\n",
      "Epoch 10/5000\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 2.0954 - accuracy: 0.7530 - val_loss: 4.7605 - val_accuracy: 0.7750\n",
      "Epoch 11/5000\n",
      " 500/1000 [==============>...............] - ETA: 0s - loss: 2.1073 - accuracy: 0.7600"
     ]
    }
   ],
   "source": [
    "history = model.fit(p_train,B_train,validation_split=0.038,batch_size=10,epochs=5000,callbacks=[callbacks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('98bk02.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('EmuBk0.2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,6))\n",
    "\n",
    "plt.suptitle(r'Accuracy and Loss on the validation set for $B(k_1,k_2,k_3)$ at $k =0.2Mpc^{-1}$')\n",
    "plt.subplot(1,2,1)\n",
    "#plt.hline()\n",
    "c = list(np.arange(0,1,0.1))\n",
    "c.append(0.93)\n",
    "plt.yticks(c)\n",
    "plt.plot(history.history['accuracy'],lw=3,color = 'purple')\n",
    "plt.plot(history.history['val_accuracy'],'.',color='orange',)\n",
    "plt.hlines(y = 0.93,xmin=0,xmax=1600,linestyles='dashdot',lw =1)\n",
    "\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['training', 'validation'], loc='lower right')\n",
    "\n",
    "# summarize history for loss\n",
    "plt.subplot(1,2,2)\n",
    "\n",
    "plt.plot(history.history['loss'],color = 'purple')\n",
    "plt.plot(history.history['val_loss'],color='orange')\n",
    "plt.hlines(y = 0,xmin=0,xmax=1600,linestyles='dashed',lw =1)\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['training', 'validation'], loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
